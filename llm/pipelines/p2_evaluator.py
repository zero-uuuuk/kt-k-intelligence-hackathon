# llm/pipelines/p2_evaluator.py

import json
import torch
import re
import gc
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import chromadb
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
try:
    from ..core.config import settings # core/config.pyì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
except ImportError:
    from core.config import settings

# --- P2ìš© í—¬í¼ í•¨ìˆ˜ ---
def load_json_file(filepath):
    """JSON íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return None

# --- P2 íŒŒì´í”„ë¼ì¸ì˜ êµ¬ì„± ìš”ì†Œë“¤ ---

class SimilarityEvaluator:
    """ë¬¸ì¥ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ì „ë‹´í•˜ëŠ” í´ë˜ìŠ¤"""
    _instance = None
    _model = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(SimilarityEvaluator, cls).__new__(cls)
        return cls._instance

    def _load_model(self):
        if self._model is None:
            print("ğŸ”„ ì§ë¬´ ìœ ì‚¬ë„ í‰ê°€ë¥¼ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            self._model = SentenceTransformer(settings.EMBEDDING_MODEL)
            print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        self._load_model()
        embeddings = self._model.encode([text1, text2])
        return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

class LLMManager:
    """P2 í‰ê°€ìš© LLM ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ (dependencies.pyì—ì„œ ê³µìœ ë¨)"""
    def __init__(self, model_name):
        self.model_name = model_name
        self.model, self.tokenizer = None, None

    def load_model(self):
        if self.model is None or self.tokenizer is None:
            import torch
            if torch.cuda.is_available():
                print(f"\n>>> LLM ëª¨ë¸({self.model_name})ì„ ë¡œë”©í•©ë‹ˆë‹¤... (4-bit ì–‘ìí™”)")
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True, bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name, quantization_config=bnb_config,
                    device_map="auto", torch_dtype=torch.bfloat16
                )
            else:
                print(f"\n>>> LLM ëª¨ë¸({self.model_name})ì„ ë¡œë”©í•©ë‹ˆë‹¤... (CPU ëª¨ë“œ)")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name, torch_dtype=torch.float32
                )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding_side='left')
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

    def generate(self, prompt: str) -> str:
        if not self.model or not self.tokenizer:
            raise RuntimeError("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(device)
        if 'token_type_ids' in inputs:
            del inputs['token_type_ids']
        output_tokens = self.model.generate(
            **inputs, max_new_tokens=512,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=True, temperature=0.1, top_p=0.9
        )
        return self.tokenizer.decode(output_tokens[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()


class QuantitativeEvaluator:
    """scoring_rules.jsonì„ ê¸°ë°˜ìœ¼ë¡œ ì •ëŸ‰ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„"""
    def __init__(self, rules_path, universities_kb_path, certifications_kb_path, similarity_evaluator):
        self.rules_data = load_json_file(rules_path)
        self.uni_kb = {uni['name']: uni['group'] for uni in load_json_file(universities_kb_path)}
        self.cert_type_map = self._build_cert_type_map(load_json_file(certifications_kb_path))
        self.similarity_evaluator = similarity_evaluator
        self.target_job_role = self.rules_data.get('common_rules', {}).get('job_role', 'Software Engineer')
        if not self.rules_data:
            raise ValueError("Scoring rules íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def _build_cert_type_map(self, cert_data):
        type_map = {}
        for cert in cert_data:
            cert_type = cert.get('type')
            if cert_type:
                type_map[cert['name'].lower()] = cert_type  
                for alias in cert.get('aliases', []):
                    type_map[alias.lower()] = cert_type
        return type_map

    def _find_answer_item(self, rule_id, applicant_data):
        """ID ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤."""
        for item in applicant_data.get('resumeItemAnswers', []):
            if item.get('resumeItemId') == rule_id:
                return item
        return {}

    def _evaluate_category(self, rule, applicant_answer_item):
        applicant_category = applicant_answer_item.get('selectedCategory')
        applicant_content = applicant_answer_item.get('resumeContent', '')

        if not applicant_category:
            if 'í•™ë ¥' in rule.get('name', ''):
                uni_name_match = re.search(r'(\w+ëŒ€í•™êµ|\w+ëŒ€)', applicant_content)
                uni_name = uni_name_match.group(1) if uni_name_match else None
                if uni_name and uni_name in self.uni_kb:
                    applicant_category = self.uni_kb[uni_name]
                else:
                    return 0
            else:
                for criterion in rule.get('criteria', []):
                    if criterion.get('description', '').strip() in applicant_content:
                        return criterion.get('score_per_grade', 0)
                return 0

        if applicant_category:
            for criterion in rule.get('criteria', []):
                criterion_desc = criterion.get('description', '')
                if applicant_category == criterion_desc or applicant_category in criterion_desc.split('Â·'):
                    return criterion.get('score_per_grade', 0)

        return 0

    def _evaluate_numeric_range(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '')
        gpa_match = re.search(r'(ì´ê³µ|ì¸ë¬¸)\s*(\d+(?:\.\d+)?)', applicant_content)
        if gpa_match:
            applicant_major, applicant_score = gpa_match.groups()
            applicant_score = float(applicant_score)
            for criterion in rule.get('criteria', []):
                desc = criterion.get('description', '')
                major_rule_match = re.search(rf'{applicant_major}\s*([<â‰¥>â‰¤])\s*(\d+(?:\.\d+)?)', desc)
                if major_rule_match:
                    operator, threshold = major_rule_match.groups()
                    threshold = float(threshold)
                    if (operator == 'â‰¥' and applicant_score >= threshold) or \
                       (operator == '<' and applicant_score < threshold):
                        return criterion.get('score_per_grade', 0)
            return 0
        return 0

    def _evaluate_hours_range(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '')
        simple_numeric_match = re.search(r'(\d+)', applicant_content)
        if not simple_numeric_match: return 0
        value = int(simple_numeric_match.group(1))

        sorted_criteria = sorted(rule.get('criteria', []), key=lambda x: int(x.get('description', 0)), reverse=True)
        for criterion in sorted_criteria:
            if value >= int(criterion.get('description', 0)):
                return criterion.get('score_per_grade', 0)
        return 0

    def _evaluate_duration_based(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '').strip()
        if not applicant_content: return 0
        duration_match = re.search(r'(\d+)\s*ê°œì›”', applicant_content)
        if not duration_match: return 0
        months = int(duration_match.group(1))
        sorted_criteria = sorted(rule.get('criteria', []), key=lambda x: int(x.get('description', 0)), reverse=True)
        for criterion in sorted_criteria:
            if months >= int(criterion.get('description', 0)):
                return criterion.get('score_per_grade', 0)
        return 0

    def _evaluate_rule_based_count(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '').lower().strip()
        applicant_certs = [c.strip() for c in applicant_content.split(',')] if applicant_content else []
        total_score = 0

        for sub_rule in rule.get('criteria', []):
            rule_cert_type = sub_rule['description']
            points_per_item = sub_rule['score_per_grade']
            max_items = sub_rule.get('max_items', float('inf'))

            count = 0
            for cert_name in applicant_certs:
                applicant_cert_type = self.cert_type_map.get(cert_name)
                if applicant_cert_type and applicant_cert_type in rule_cert_type:
                    count += 1

            score_for_rule = min(count, max_items) * points_per_item
            total_score += score_for_rule

        return min(total_score, rule.get('score_weight', total_score))


    def _evaluate_score_range(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '').strip().lower()
        if not applicant_content:
            return 0

        # ì§€ì›ì ì…ë ¥ì„ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„ë¦¬ (ì˜ˆ: "opic ih" â†’ ["opic", "ih"])
        applicant_tokens = applicant_content.split()

        for criterion in rule.get('criteria', []):
            criterion_desc = criterion.get('description', '').lower()
            criterion_tokens = criterion_desc.split()

            # ì§€ì›ìì˜ ëª¨ë“  í† í°ì´ ê¸°ì¤€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if all(token in criterion_tokens for token in applicant_tokens):
                return criterion.get('score_per_grade', 0)

        return 0

    def _extract_career_info(self, career_content):
        """ê²½ë ¥ ì •ë³´ì—ì„œ ì§ë¬´ëª…ê³¼ ê¸°ê°„ ì¶”ì¶œ"""
        parts = [part.strip() for part in career_content.split(',')]
    
        company = parts[0] if len(parts) > 0 else ""
        job_title = parts[1] if len(parts) > 1 else ""
        duration_text = parts[2] if len(parts) > 2 else ""
    
        # ê¸°ê°„ ì¶”ì¶œ (14ê°œì›” â†’ 14)
        duration_match = re.search(r'(\d+)', duration_text)
        duration_months = int(duration_match.group(1)) if duration_match else 0
    
        return {
            "company": company,
            "job_title": job_title,
            "duration_months": duration_months
        }

    def _calculate_job_similarity(self, applicant_job, target_job):
        """ë‘ ì§ë¬´ëª… ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not applicant_job or not target_job:
            return 0.0
        
        # ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°í™”
        applicant_embedding = self.embedding_model.encode([applicant_job])
        target_embedding = self.embedding_model.encode([target_job])
    
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(applicant_embedding, target_embedding)[0][0]
        return similarity

    def _evaluate_career_similarity_based(self, rule, applicant_answer_item):
        """ì§ë¬´ ìœ ì‚¬ë„ë¥¼ ê³ ë ¤í•œ ê²½ë ¥ í‰ê°€"""
        career_content = applicant_answer_item.get("resumeContent", "").strip()
        if not career_content:
            return 0
    
        career_info = self._extract_career_info(career_content)
    
        # ì§ë¬´ ìœ ì‚¬ë„ ê³„ì‚°
        target_job_role = rule.get("target_job_role", "")
        similarity = self._calculate_job_similarity(
            career_info["job_title"], 
            target_job_role
        )
    
        print(f"ì§ë¬´ ìœ ì‚¬ë„: {career_info['job_title']} vs {target_job_role} = {similarity:.3f}")
    
        # ìœ ì‚¬ë„ ì„ê³„ê°’ í™•ì¸
        similarity_threshold = rule.get("similarity_threshold", 0.7)
        if similarity < similarity_threshold:
            print(f"ìœ ì‚¬ë„ {similarity:.3f} < ì„ê³„ê°’ {similarity_threshold}, ê°ì  ì ìš©")
            return min(8, self._calculate_duration_score(career_info["duration_months"], rule.get("criteria", [])) * 0.5)
    
        # ìœ ì‚¬ ì§ë¬´ì¸ ê²½ìš° ê¸°ê°„ì— ë”°ë¼ ì •ìƒ ì ìˆ˜ ë¶€ì—¬
        duration_score = self._calculate_duration_score(career_info["duration_months"], rule.get("criteria", []))
        print(f"ìœ ì‚¬ì§ë¬´ í™•ì¸, ê¸°ê°„ {career_info['duration_months']}ê°œì›”ë¡œ {duration_score}ì  ë¶€ì—¬")
    
        return duration_score

    def _calculate_duration_score(self, duration_months, criteria):
        """ê¸°ê°„ì— ë”°ë¥¸ ì ìˆ˜ ê³„ì‚°"""
        sorted_criteria = sorted(criteria, key=lambda x: int(x.get("description", "0")), reverse=True)
        for criterion in sorted_criteria:
            if duration_months >= int(criterion.get("description", "0")):
                return criterion.get("score_per_grade", 0)
        return 0

    def evaluate(self, applicant_data):
        """[ì˜¤ë¥˜ ìˆ˜ì •] IDë¥¼ keyë¡œ ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜"""
        results_by_id = {}
        for rule in self.rules_data.get('resume_items', []):
            rule_id, item_type = rule['id'], rule['type']
            answer_item = self._find_answer_item(rule_id, applicant_data)
            score = 0
            if answer_item:
                if item_type == 'CATEGORY': 
                    score = self._evaluate_category(rule, answer_item)
                elif item_type == 'NUMERIC_RANGE': 
                    score = self._evaluate_numeric_range(rule, answer_item)
                elif item_type == 'HOURS_RANGE': 
                    score = self._evaluate_hours_range(rule, answer_item)
                elif item_type == 'DURATION_BASED': 
                    score = self._evaluate_duration_based(rule, answer_item)
                elif item_type == 'CAREER_SIMILARITY_BASED':  # ìƒˆë¡œ ì¶”ê°€
                    score = self._evaluate_career_similarity_based(rule, answer_item)
                elif item_type == 'RULE_BASED_COUNT': 
                    score = self._evaluate_rule_based_count(rule, answer_item)
                elif item_type == 'SCORE_RANGE': 
                    score = self._evaluate_score_range(rule, answer_item)
            results_by_id[rule_id] = score
        return results_by_id


class QualitativeEvaluator:
    """RAGì™€ LLMì„ ì‚¬ìš©í•˜ì—¬ ì •ì„± í‰ê°€ ë° ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„"""
    def __init__(self, rag_data_path, db_path, collection_name, llm_manager):
        self.rag_criteria = self._load_rag_criteria(rag_data_path)
        self.llm_manager = llm_manager
        self.db_client = chromadb.PersistentClient(path=db_path)
        self.collection = self.db_client.get_collection(
            name=collection_name,
            embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(model_name=settings.EMBEDDING_MODEL)
        )

    def _load_rag_criteria(self, rag_data_path):
        all_rag_data = load_json_file(rag_data_path)
        criteria = {}
        for item in all_rag_data:
            if item.get("type") == "criterion":
                q_id = item["question_id"]
                if q_id not in criteria:
                    criteria[q_id] = []
                criteria[q_id].append(item["content"])
        return criteria

    def _search_examples(self, question_id, applicant_answer):
        results = self.collection.query(
            query_texts=[applicant_answer], n_results=3,
            where={"question_id": question_id}
        )
        return results['documents'][0] if (results and results['documents']) else []

    def _create_prompt(self, prompt_type, **kwargs):
        if prompt_type == "item_evaluation":
            excellent_desc = next((d['description'] for d in kwargs['criterion']['details'] if d['grade'] == 'EXCELLENT'), "N/A")
            return f"""### ì§€ì‹œ:
ë‹¹ì‹ ì€ ì‹ ì… BE ê°œë°œì ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. [ì§€ì›ì ë‹µë³€]ì„ [í‰ê°€ ê¸°ì¤€]ê³¼ [ìœ ì‚¬ ì˜ˆì‹œ]ë¥¼ ì°¸ê³ í•˜ì—¬ í‰ê°€í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### í‰ê°€ ëŒ€ìƒ:
[ì§€ì›ì ë‹µë³€]
{kwargs['applicant_answer']}

### í‰ê°€ ê¸°ì¤€: {kwargs['criterion']['name']}
- **ìµœê³ ì (EXCELLENT)**: {excellent_desc}

### ì°¸ê³  ìë£Œ:
[ìœ ì‚¬ ì˜ˆì‹œ]
{kwargs['examples_text']}

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ ì¤€ìˆ˜, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€):
{{
  "evaluatedContent": "[ì§€ì›ì ë‹µë³€]ì—ì„œ [í‰ê°€ ê¸°ì¤€]ê³¼ ê°€ì¥ ê´€ë ¨ ê¹Šì€ í•µì‹¬ ë¬¸ì¥ 1ê°œë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œ. ì—†ë‹¤ë©´ 'ì—†ìŒ'ìœ¼ë¡œ ì‘ë‹µ.",
  "grade": "í‰ê°€ ê²°ê³¼ë¥¼ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€.",
  "evaluationReason": "í‰ê°€ ê·¼ê±°ë¥¼ [í‰ê°€ ê¸°ì¤€]ê³¼ ì—°ê´€ ì§€ì–´ 1ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ ."
}}

### ì¶œë ¥:
"""
        elif prompt_type == "question_summary":
            return f"""### ì§€ì‹œ:
ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ë¬¸í•­ ë‹µë³€]ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### í‰ê°€ ëŒ€ìƒ:
[ë¬¸í•­ ë‹µë³€]
{kwargs['answer_content']}

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ ì¤€ìˆ˜, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€):
{{
  "keywords": ["ë‹µë³€ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±"],
  "summary": "ë‹µë³€ ë‚´ìš©ì„ 1~2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½."
}}

### ì¶œë ¥:
"""
        elif prompt_type == "overall_analysis":
            return f"""### ì§€ì‹œ:
ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ì§€ì›ì ì¢…í•© ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§€ì›ìë¥¼ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### ë¶„ì„ ëŒ€ìƒ:
[ì§€ì›ì ì¢…í•© ì •ë³´]
{kwargs['total_report']}

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ ì¤€ìˆ˜, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€):
{{
  "overallEvaluation": "ì§€ì›ìì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.",
  "strengths": ["ì§€ì›ìì˜ ê°•ì ì„ 3ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±"],
  "improvements": ["ì§€ì›ìì˜ ê°œì„ ì ì„ 2ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±"],
  "aiRecommendation": "ì¢…í•©ì ì¸ íŒë‹¨ì— ë”°ë¼ 'í•©ê²© ê¶Œì¥' ë˜ëŠ” 'ì‹ ì¤‘í•œ ê²€í†  í•„ìš”' ë˜ëŠ” 'íƒˆë½ ê¶Œì¥' ì¤‘ í•˜ë‚˜ë¡œ ê²°ë¡ .",
  "aiReliability": "í˜„ì¬ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ AIì˜ ì‹ ë¢°ë„ë¥¼ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì†Œìˆ˜ì  ë‘ ìë¦¬ ìˆ«ìë¡œ í‘œí˜„. (ì˜ˆ: 0.87)"
}}

### ì¶œë ¥:
"""

    def _call_llm(self, prompt):
        raw_response = self.llm_manager.generate(prompt)
        try:
            # JSON ê°ì²´ê°€ ì‹œì‘í•˜ëŠ” ì²« '{'ë¥¼ ì°¾ì•„ ê·¸ ì´í›„ì˜ ëª¨ë“  ë‚´ìš©ì„ íŒŒì‹±
            json_part = raw_response[raw_response.find('{'):]
            return json.loads(json_part)
        except (json.JSONDecodeError, AttributeError):
            return None # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

    def evaluate(self, applicant_data, quant_results): # [ì˜¤ë¥˜ í•´ê²°] quant_results ì¸ì ì¶”ê°€
        self.llm_manager.load_model()
        cover_letter_evals = []
        answers = applicant_data.get('coverLetterQuestionAnswers', [])
        for answer_item in answers:
            q_id = answer_item['coverLetterQuestionId']  # ì´ë¯¸ camelCase
            answer_content = answer_item['answerContent']  # ì´ë¯¸ camelCase
            answer_evaluations = []
            criteria_for_question = self.rag_criteria.get(q_id, [])
            similar_examples = self._search_examples(q_id, answer_content)
            examples_text = "\n".join([f"- {ex}" for ex in similar_examples]) if similar_examples else "ì—†ìŒ"
            for criterion in tqdm(criteria_for_question, desc=f"  - Q{q_id} ì •ì„± í‰ê°€ ì¤‘"):
                prompt = self._create_prompt("item_evaluation", applicant_answer=answer_content, criterion=criterion, examples_text=examples_text)
                eval_result = self._call_llm(prompt)
                if eval_result:
                    eval_result["evaluationCriteriaName"] = criterion['name']
                    answer_evaluations.append(eval_result)
            summary_prompt = self._create_prompt("question_summary", answer_content=answer_content)
            summary_result = self._call_llm(summary_prompt) or {"keywords": [], "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"}
            cover_letter_evals.append({
                "coverLetterQuestionId": q_id,
                "keywords": summary_result.get('keywords', []),
                "summary": summary_result.get('summary', ''),
                "answerEvaluations": answer_evaluations
            })
        temp_report = {"ì •ëŸ‰ í‰ê°€": quant_results, "ì •ì„± í‰ê°€": cover_letter_evals}
        overall_prompt = self._create_prompt("overall_analysis", total_report=json.dumps(temp_report, ensure_ascii=False, indent=2))
        overall_analysis = self._call_llm(overall_prompt) or {
            "overallEvaluation": "ë¶„ì„ ì‹¤íŒ¨", "strengths": [], "improvements": [],
            "aiRecommendation": "íŒë‹¨ ë¶ˆê°€", "aiReliability": 0.0
        }
        return cover_letter_evals, overall_analysis


def run_p2_pipeline(applicant_data: dict, llm_manager: LLMManager, similarity_evaluator: SimilarityEvaluator):
    """
    P2 íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    print(f"--- ğŸ§  1. ì •ëŸ‰ í‰ê°€ ì‹œì‘: {applicant_data.get('applicantName')} ---")
    quant_evaluator = QuantitativeEvaluator(
        rules_path=settings.SCORING_RULES_FILE,
        universities_kb_path=settings.UNIVERSITIES_KB_FILE,
        certifications_kb_path=settings.CERTIFICATIONS_KB_FILE,
        similarity_evaluator=similarity_evaluator
    )
    quant_scores_by_id = quant_evaluator.evaluate(applicant_data)

    print(f"--- ğŸ–‹ï¸ 2. ì •ì„± í‰ê°€ ë° ì¢…í•© ë¶„ì„ ì‹œì‘ ---")
    qual_evaluator = QualitativeEvaluator(
        rag_data_path=settings.RAG_DATA_FILE,
        db_path=settings.DB_PATH,
        collection_name=settings.COLLECTION_NAME,
        llm_manager=llm_manager # ê³µìœ ëœ LLM ë§¤ë‹ˆì € ì‚¬ìš©
    )
    
    cover_letter_evals, overall_analysis = qual_evaluator.evaluate(
        applicant_data, {"scores_by_id": quant_scores_by_id}
    )

    print(f"--- ğŸ“ 3. ìµœì¢… í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ---")
    resume_evaluations = []
    for answer in applicant_data.get('resumeItemAnswers', []):
        item_id = answer['resumeItemId']
        content = answer.get('resumeContent', '') or answer.get('selectedCategory', '')
        resume_evaluations.append({
            "resumeItemId": item_id,
            "resumeItemName": answer['resumeItemName'],
            "resumeContent": content,
            "score": quant_scores_by_id.get(item_id, 0)
        })
    
    final_report = {
        "applicantId": applicant_data['applicantId'],
        "applicantName": applicant_data['applicantName'],
        "applicantEmail": applicant_data['applicantEmail'],
        "applicationId": applicant_data['applicationId'],
        "jobPostingId": applicant_data['jobPostingId'],
        "resumeEvaluations": resume_evaluations,
        "coverLetterQuestionEvaluations": cover_letter_evals,
        "overallAnalysis": overall_analysis
    }
    
    return final_report