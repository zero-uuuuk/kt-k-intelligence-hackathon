# llm/pipelines/p1_builder.py

import json
import torch
import os
import gc
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
import chromadb
# core/config.pyì—ì„œ ì„¤ì •ê°’ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ import
try:
    from ..core.config import settings
except ImportError:
    from core.config import settings

# --- P1ìš© í—¬í¼ í•¨ìˆ˜ ---
def load_json_file(filepath):
    """JSON íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš° Noneì„ ë°˜í™˜í•˜ì—¬ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
        return None

# --- P1 íŒŒì´í”„ë¼ì¸ì˜ êµ¬ì„± ìš”ì†Œë“¤ ---

class LLMManager:
    """
    P1 íŒŒì´í”„ë¼ì¸ì—ì„œ RAG ë°ì´í„° ìƒì„±ì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” LLM ê´€ë¦¬ í´ë˜ìŠ¤
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(LLMManager, cls).__new__(cls)
            cls._instance.model, cls._instance.tokenizer = None, None
        return cls._instance
    
    def __init__(self, model_name):
        self.model_name = model_name

    def load_model(self):
        if self.model is None or self.tokenizer is None:
            import torch
            print(f">>> P1 LLM ëª¨ë¸({self.model_name})ì„ ë¡œë”©í•©ë‹ˆë‹¤. (CUDA ëª¨ë“œ)")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name, 
                device_map="auto", 
                torch_dtype=torch.bfloat16
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding_side='left')
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print("âœ… P1 LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    
    def refine_with_original(self, generated_clue: str, original_text: str) -> str:
        """LLMì˜ ë‹µë³€(ë‹¨ì„œ)ì„ ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ëŒ€ì¡°í•˜ì—¬ ì™„ë²½í•œ ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        clue = re.sub(r'.*?ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤[:\s]*', '', generated_clue, flags=re.DOTALL)
        clue = re.sub(r'["â€œ]', '', clue).strip()
        original_sentences = re.split(r'(?<=[.!?])\s+', original_text)
        key_phrases = clue.split()
        if len(key_phrases) < 3: return ""
        for sentence in original_sentences:
            start_phrase = ' '.join(key_phrases[:5])
            if start_phrase in sentence:
                return sentence.strip()
        best_match_sentence, highest_match_score = "", 0
        for sentence in original_sentences:
            match_count = sum(1 for word in key_phrases if word in sentence)
            score = match_count / len(key_phrases)
            if score > highest_match_score and score > 0.5:
                highest_match_score = score
                best_match_sentence = sentence.strip()
        return best_match_sentence

    def generate_batch(self, prompts: list[str]) -> list[str]:
        """ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ 'ë‹¨ì„œ'ê°€ ë  ë¬¸ì¥ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.model or not self.tokenizer: raise RuntimeError("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        inputs = self.tokenizer(prompts, padding=True, return_tensors="pt", truncation=True, max_length=1024).to("cuda")
        if 'token_type_ids' in inputs: del inputs['token_type_ids']
        input_token_len = inputs["input_ids"].shape[1]
        output_tokens = self.model.generate(
            **inputs, max_new_tokens=150, eos_token_id=self.tokenizer.eos_token_id,
            do_sample=True, temperature=0.1, top_p=0.9
        )
        return [
            self.tokenizer.decode(output[input_token_len:], skip_special_tokens=True).strip()
            for output in output_tokens
        ]


class P1_AssetBuilder:
    """
    í‰ê°€ ê¸°ì¤€ê³¼ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€ ìì‚°(scoring_rules, rag_data)ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, criteria_data: dict, examples_data: dict, llm_manager: LLMManager):
        self.eval_criteria = criteria_data
        self.examples = examples_data
        self.llm_manager = llm_manager
        self.llm_call_stats = {}

    def generate_scoring_rules(self, output_path: str):
        """ì •ëŸ‰ í‰ê°€ ê·œì¹™(scoring_rules.json)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        print("[P1] ì •ëŸ‰ í‰ê°€ ê·œì¹™ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        scoring_rules = {
            "common_rules": self.eval_criteria.get("commonRules", {}),
            "resume_items": self.eval_criteria.get("resumeItems", [])  # camelCaseë¡œ ë³€ê²½
        }
        
        # 'jobRole'ì´ ìˆìœ¼ë©´, ê²½ë ¥ í‰ê°€ ê·œì¹™ì„ 'ìœ ì‚¬ë„ ê¸°ë°˜'ìœ¼ë¡œ ë™ì  ë³€ê²½
        job_role = scoring_rules.get("common_rules", {}).get("jobRole")  # camelCaseë¡œ ë³€ê²½
        if job_role:
            for item in scoring_rules.get("resume_items", []):
                if item.get("name") == "ê²½ë ¥":
                    print(f"  - ì±„ìš© ì§ë¬´('{job_role}')ê°€ í™•ì¸ë˜ì–´, ê²½ë ¥ í‰ê°€ ë°©ì‹ì„ 'ì§ë¬´ ìœ ì‚¬ë„ ê¸°ë°˜'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                    item["type"] = "CAREER_SIMILARITY_BASED"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(scoring_rules, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì •ëŸ‰ í‰ê°€ ê·œì¹™ ìƒì„± ì™„ë£Œ: {output_path}")

    def _create_extraction_prompt(self, criterion: dict, example_content: str, job_info: dict) -> str:
        criterion_name = criterion['name']
        excellent_desc = next((d['description'] for d in criterion['details'] if d['grade'] == 'EXCELLENT'), "")
        poor_desc = next((d['description'] for d in criterion['details'] if d['grade'] == 'POOR'), "")
        return f"""### ì§€ì‹œ:
ì£¼ì–´ì§„ [ìê¸°ì†Œê°œì„œ ì›ë¬¸]ì—ì„œ '{criterion_name}' ê¸°ì¤€ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì¥ 1ê°œë§Œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

[ì±„ì  ê°€ì´ë“œ]
- ìµœê³ ì (EXCELLENT) ë¬¸ì¥ì˜ íŠ¹ì§•: {excellent_desc}
- ìµœì €ì (POOR) ë¬¸ì¥ì˜ íŠ¹ì§•: {poor_desc}

[ì¶”ì¶œ ê·œì¹™]
- ë¶€ê°€ ì„¤ëª…, ì¤„ë°”ê¿ˆ, ë”°ì˜´í‘œ ì—†ì´ ì˜¤ì§ ì¶”ì¶œí•  ë¬¸ì¥ë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
- ì í•©í•œ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ "ì¶”ì¶œë¶ˆê°€" ë¼ê³ ë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

### ì…ë ¥:
[ìê¸°ì†Œê°œì„œ ì›ë¬¸]
{example_content}

### ì¶œë ¥:
[ì¶”ì¶œ ë¬¸ì¥]
"""

    def generate_rag_data(self, output_path: str):
        """RAGì— ì‚¬ìš©ë  ë°ì´í„°(rag_data.json)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("[P1] RAG ë°ì´í„° ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        BATCH_SIZE = 8
        rag_data = []
        
        print("  - (1/3) í‰ê°€ ê¸°ì¤€(criterion) ë°ì´í„°ë¥¼ ìƒì„± ì¤‘...")
        criteria_packets = [
            {"packet_id": f"Q{q['id']}_{c['name']}", "type": "criterion", "question_id": q['id'], "content": c}
            for q in self.eval_criteria.get("coverLetterQuestions", []) for c in q.get("criteria", [])  # camelCaseë¡œ ë³€ê²½
        ]
        rag_data.extend(criteria_packets)
        print(f"  - í‰ê°€ ê¸°ì¤€ {len(criteria_packets)}ê°œ ìƒì„± ì™„ë£Œ.")
        print("  - (2/3) LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ì‘ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        job_info = {"companyName": self.eval_criteria.get("companyName", "íšŒì‚¬"), "title": self.eval_criteria.get("title", "ì‹ ì…ì‚¬ì›")}  # camelCaseë¡œ ë³€ê²½
        tasks_to_process = [
            {"crit_pack": crit_pack, "example": ex, "label": label}
            for crit_pack in criteria_packets
            for ex_group in self.examples if ex_group["question_id"] == crit_pack["question_id"]
            for label, key in [("good", "good_examples"), ("bad", "bad_examples")]
            for ex in ex_group.get(key, [])
        ]
        print(f"  - ì´ {len(tasks_to_process)}ê°œì˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‘ì—…ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
        print(f"  - (3/3) ì‘ì—…ì„ ë¯¸ë‹ˆ ë°°ì¹˜(í¬ê¸°: {BATCH_SIZE})ë¡œ ë‚˜ëˆ„ì–´ LLMìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

        progress_bar = tqdm(range(0, len(tasks_to_process), BATCH_SIZE), desc="  - ì „ì²´ ì§„í–‰ë¥ ")
        for i in progress_bar:
            batch_tasks = tasks_to_process[i:i + BATCH_SIZE]
            prompts = [self._create_extraction_prompt(t["crit_pack"]['content'], t["example"]['content'], job_info) for t in batch_tasks]

            raw_sentences = self.llm_manager.generate_batch(prompts)

            for task, raw_sentence in zip(batch_tasks, raw_sentences):
                original_content = task["example"]["content"]
                # [ìµœì¢… í•´ê²°ì±…] ì›ë³¸ ëŒ€ì¡°ë¥¼ í†µí•´ ìµœì¢… ë¬¸ì¥ ì •ì œ
                final_sentence = self.llm_manager.refine_with_original(raw_sentence, original_content)

                criterion_name = task["crit_pack"]["content"]["name"]
                log_msg = f"  [ì‹¤ì‹œê°„] ê¸°ì¤€: {criterion_name:<10s} | ì˜ˆì‹œ: {task['example']['id']:<12s} -> \"{final_sentence[:40]}...\""
                progress_bar.write(log_msg)

                self.llm_call_stats.setdefault(criterion_name, {'success': 0, 'fail': 0})
                if final_sentence and "ì¶”ì¶œë¶ˆê°€" not in final_sentence:
                    rag_data.append({
                        "packet_id": f"EX_{task['example']['id']}_{criterion_name}", "type": "example_sentence", "question_id": task["crit_pack"]["question_id"],
                        "content": {"label": task["label"], "linked_criterion": criterion_name, "extracted_sentence": final_sentence}
                    })
                    self.llm_call_stats[criterion_name]['success'] += 1
                else:
                    self.llm_call_stats[criterion_name]['fail'] += 1

            gc.collect()
            torch.cuda.empty_cache()

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(rag_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… RAG ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_path}")


def upload_rag_data_to_db(rag_data_path: str):
    """ìƒì„±ëœ rag_data.jsonì„ ChromaDBì— ì—…ë¡œë“œí•˜ê³  ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜"""
    print(f"ğŸš€ ChromaDBì— RAG ë°ì´í„° ì—…ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: {settings.DB_PATH}")
    rag_data = load_json_file(rag_data_path)
    if not rag_data:
        print("âš ï¸ RAG ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("í˜„ì¬ rag_data_path: ", rag_data_path)
        return

    client = chromadb.PersistentClient(path=settings.DB_PATH)
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆë‹¤ë©´ ì‚­ì œí•˜ì—¬ í•­ìƒ ìµœì‹  ë°ì´í„°ë¡œ ìœ ì§€
    client.delete_collection(name=settings.COLLECTION_NAME)
    collection = client.create_collection(
        name=settings.COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"},
        embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=settings.EMBEDDING_MODEL
        )
    )
    
    # --- 6. ì—…ë¡œë“œí•  ë°ì´í„° ì¤€ë¹„ ---
    documents = []
    metadatas = []
    ids = []

    for item in rag_data:
        if item.get("type") == "example_sentence":
            content = item["content"]
            documents.append(content["extracted_sentence"])
            metadatas.append({
                "question_id": item["question_id"],
                "label": content["label"],
                "linked_criterion": content["linked_criterion"]
            })
            ids.append(item["packet_id"])

    # --- 7. ë°ì´í„° ì—…ë¡œë“œ ì‹¤í–‰ ---
    if documents:
        print(f"\nğŸš€ ì´ {len(documents)}ê°œì˜ ëŒ€í‘œ ë¬¸ì¥ì„ ChromaDBì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        print("ğŸ‰ ì—…ë¡œë“œ ë° ì„ë² ë”© ì™„ë£Œ!")
        print(f"'{settings.DB_PATH}' í´ë”ì— DB íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   (Google Drive 'KTí•´ì»¤í†¤' í´ë”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.)")
    else:
        print("âš ï¸ ì—…ë¡œë“œí•  ëŒ€í‘œ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")

        print(f"ğŸ‰ ì´ {len(documents)}ê°œì˜ ëŒ€í‘œ ë¬¸ì¥ì„ ChromaDBì— ì—…ë¡œë“œ ë° ì„ë² ë”© ì™„ë£Œ!")


# main.pyì—ì„œ í˜¸ì¶œí•  ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_p1_pipeline(eval_criteria_data: dict, examples_data: dict):
    """P1 íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í•¨ìˆ˜"""
    
    # 1. P1 ë¹Œë” ì‹¤í–‰í•˜ì—¬ ìì‚° íŒŒì¼ ìƒì„±
    llm_manager = LLMManager(model_name=settings.MODEL_NAME)
    llm_manager.load_model()
    
    builder = P1_AssetBuilder(
        criteria_data=eval_criteria_data,
        examples_data=examples_data,
        llm_manager=llm_manager
    )
    builder.generate_scoring_rules(settings.SCORING_RULES_FILE)
    builder.generate_rag_data(settings.RAG_DATA_FILE)
    
    # 2. ìƒì„±ëœ RAG ë°ì´í„°ë¥¼ ChromaDBì— ì—…ë¡œë“œ
    upload_rag_data_to_db(settings.RAG_DATA_FILE)